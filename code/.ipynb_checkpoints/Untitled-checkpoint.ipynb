{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad46334a-40a5-42c1-bb0f-32712f6dca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raouf\\anaconda3\\envs\\ppdenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from few_shot_clustering.wrappers import LLMPairwiseClustering\n",
    "\n",
    "from few_shot_clustering.dataloaders import load_clinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f8e2318-b9f4-413e-abb8-934d575dc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"./tmp/clinc_feature_cache.pkl\"\n",
    "features, labels, documents = load_clinc(cache_path)\n",
    "\n",
    "prompt_suffix = \"express the same general intent?\"\n",
    "text_type = \"Utterance\"\n",
    "\n",
    "prompt = \"\"\"You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
    "\n",
    "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
    "\n",
    "Utterance #1: what's the spanish word for pasta\n",
    "Utterance #2: how would they say butter in zambia\n",
    "\n",
    "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
    "\n",
    "Utterance #1: roll those dice once\n",
    "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
    "\n",
    "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
    "\n",
    "Utterance #1: how soon milk expires\n",
    "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
    "\n",
    "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
    "\n",
    "Utterance #1: nice seeing you bye\n",
    "Utterance #2: what was the date of my last car appointment\n",
    "\n",
    "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebaf37fe-007f-49cb-b0a7-06332e7fcf68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = features[30:50]\n",
    "labels = labels[:10]\n",
    "documents = documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1db6c6f-f928-4d1e-9089-d8eb7ec8cb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[60:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07d0aaa3-f637-4be1-b6e4-876d76a0bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "\n",
    "# Number of items to sample\n",
    "sample_size = 100\n",
    "\n",
    "# Take a sample from the original list\n",
    "labels = random.sample(labels, sample_size)\n",
    "\n",
    "# Sample indexes\n",
    "indexes = random.sample(range(len(labels)), sample_size)\n",
    "\n",
    "# Sample elements from each list using the sampled indexes\n",
    "labels = [labels[i] for i in indexes]\n",
    "features = [features[i] for i in indexes]\n",
    "documents = [documents[i] for i in indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9919094-8a5a-4362-920c-4cba3e198a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Constraints\n",
      "9\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: what's the spanish word for pasta\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.47 seconds\n",
      "labels:\n",
      "[False, False, False, False, True]\n",
      "\n",
      "\n",
      "Consolidate Count: 1\n",
      "8\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: how do they say tacos in mexico\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.45 seconds\n",
      "labels:\n",
      "[False, True, False, False, True]\n",
      "\n",
      "\n",
      "Consolidate Count: 2\n",
      "7\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: how would you say fly in italian\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.3 seconds\n",
      "labels:\n",
      "[True, True, True, False, True]\n",
      "\n",
      "\n",
      "Consolidate Count: 3\n",
      "6\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: what would the word for grass be in finland\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.61 seconds\n",
      "labels:\n",
      "[False, False, False, True, False]\n",
      "\n",
      "\n",
      "Consolidate Count: 4\n",
      "5\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.32 seconds\n",
      "labels:\n",
      "[False, False, False, False, False]\n",
      "\n",
      "\n",
      "Consolidate Count: 5\n",
      "4\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: how would one say cruiser in china\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.27 seconds\n",
      "labels:\n",
      "[False, False, False, False, False]\n",
      "\n",
      "\n",
      "Consolidate Count: 6\n",
      "3\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: what's the french word you use for potato\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.45 seconds\n",
      "labels:\n",
      "[False, False, False, False, False]\n",
      "\n",
      "\n",
      "Consolidate Count: 7\n",
      "2\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: how does one say wonderful in german\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.45 seconds\n",
      "labels:\n",
      "[False, True, True, False, False]\n",
      "\n",
      "\n",
      "Consolidate Count: 8\n",
      "1\n",
      "PROMPT:\n",
      "You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
      "\n",
      "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\n",
      "\n",
      "Utterance #1: what's the spanish word for pasta\n",
      "Utterance #2: how would they say butter in zambia\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: roll those dice once\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how soon milk expires\n",
      "Utterance #2: can you roll an eight sided die and tell me what it comes up as\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? Yes\n",
      "\n",
      "Utterance #1: nice seeing you bye\n",
      "Utterance #2: what was the date of my last car appointment\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? No\n",
      "\n",
      "Utterance #1: how do you say fast in spanish\n",
      "Utterance #2: what's the word for trees in norway\n",
      "\n",
      "Given this context, do utterance #1 and utterance #2 likely express the same general intent? \n",
      "Response took 0.31 seconds\n",
      "labels:\n",
      "[True, False, False, True, False]\n",
      "\n",
      "\n",
      "Consolidate Count: 9\n",
      "broke\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      3\u001b[0m documents \u001b[38;5;241m=\u001b[39m documents[:\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m cluster_assignments, constraints \u001b[38;5;241m=\u001b[39m \u001b[43mLLMPairwiseClustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_feedback_given\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpckmeans_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmp/clinc_cache_file.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraint_selection_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSimilarityFinder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk-means++\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Studies\\M2 MLSD\\ppd\\few-shot-clustering\\few_shot_clustering\\wrappers.py:75\u001b[0m, in \u001b[0;36mLLMPairwiseClustering\u001b[1;34m(features, documents, num_clusters, prompt, text_type, prompt_suffix, max_feedback_given, pckmeans_w, cache_file, constraint_selection_algorithm, kmeans_init)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     active_learner \u001b[38;5;241m=\u001b[39m DistanceBasedSelector(n_clusters\u001b[38;5;241m=\u001b[39mnum_clusters)\n\u001b[1;32m---> 75\u001b[0m \u001b[43mactive_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m pairwise_constraints \u001b[38;5;241m=\u001b[39m active_learner\u001b[38;5;241m.\u001b[39mpairwise_constraints_\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining PCKMeans\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\Studies\\M2 MLSD\\ppd\\few-shot-clustering\\few_shot_clustering\\active_semi_supervised_clustering\\active_semi_clustering\\active\\pairwise_constraints\\min_max.py:66\u001b[0m, in \u001b[0;36mSimilarityFinder.fit\u001b[1;34m(self, X, oracle)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], []\n\u001b[0;32m     65\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_explore(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters, oracle)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairwise_constraints_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighborhoods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moracle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\Studies\\M2 MLSD\\ppd\\few-shot-clustering\\few_shot_clustering\\active_semi_supervised_clustering\\active_semi_clustering\\active\\pairwise_constraints\\min_max.py:119\u001b[0m, in \u001b[0;36mSimilarityFinder._consolidate\u001b[1;34m(self, neighborhoods, X, oracle)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbroke\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(remaining_distances\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m--> 119\u001b[0m closest_point \u001b[38;5;241m=\u001b[39m remaining_list[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining_distances\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    121\u001b[0m oracle_response \u001b[38;5;241m=\u001b[39m oracle\u001b[38;5;241m.\u001b[39mquery(member, closest_point)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m oracle_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ppdenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ppdenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "features = features[:10]\n",
    "labels = labels[:10]\n",
    "documents = documents[:10]\n",
    "cluster_assignments, constraints = LLMPairwiseClustering(features, documents, len(set(labels)) , prompt, text_type, prompt_suffix, max_feedback_given=10000, pckmeans_w=0.01, cache_file=\"tmp/clinc_cache_file.json\", constraint_selection_algorithm=\"SimilarityFinder\", kmeans_init=\"k-means++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd18908-aceb-4601-8018-191cbf39ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from few_shot_clustering.eval_utils import cluster_acc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ae34f-a6e3-41a2-9d21-0f08ba4ca1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Accuracy: {cluster_acc(np.array(cluster_assignments), np.array(labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0ca7d5-4397-4767-b1c2-0b470266e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "OpenAI.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "message = completion.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c104c7db-14f1-4d19-b79d-e47176febec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"a\"\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb185bb-3657-48f5-bbdc-5a90055ca897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def some_function():\n",
    "    x = 5\n",
    "    y = 10\n",
    "    z = x + y\n",
    "    breakpoint()  # Debugger will be invoked here\n",
    "    print(z)\n",
    "\n",
    "some_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9812bd26-fb1d-4ec3-a2b6-496dacfb11f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "I am trying to cluster task-oriented dialog system queries based on whether they express the same general user intent. To help me with this, for a given user query, provide a comprehensive set of keyphrases that could describe this query's intent. These keyphrases should be distinct from those that might describe queries with different intents. Generate the set of keyphrases as a JSON-formatted list.\n",
      "\n",
      "Query: \"how would you say fly in italian\"\n",
      "\n",
      "Keyphrases: [\"translation\", \"translate\"]\n",
      "\n",
      "Query: \"what does assiduous mean\"\n",
      "\n",
      "Keyphrases: [\"definition\", \"define\"]\n",
      "\n",
      "Query: \"find my cellphone for me!\"\n",
      "\n",
      "Keyphrases: [\"location\", \"find\", \"locate\", \"tracking\", \"track\"]\n",
      "\n",
      "Query: \"how would you say fly in italian\"\n",
      "\n",
      "Keyphrases:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [2:12:43, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\Studies\\M2 MLSD\\ppd\\few-shot-clustering\\few_shot_clustering\\active_semi_supervised_clustering\\active_semi_clustering\\semi_supervised\\pairwise_constraints\\gptclustering.py:115\u001b[0m, in \u001b[0;36mGPTExpansionClustering.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    114\u001b[0m deployment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT-3-5-turbo-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 115\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    116\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    117\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    118\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: template_to_fill},\n\u001b[0;32m    119\u001b[0m     ],\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    122\u001b[0m message \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mstr\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ppdenv\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels[:\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     36\u001b[0m documents \u001b[38;5;241m=\u001b[39m documents[:\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m---> 38\u001b[0m cluster_assignments \u001b[38;5;241m=\u001b[39m \u001b[43mLLMKeyphraseClustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_for_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRepresent keyphrases for topic classification:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tmp/clinc_expansion_cache_file.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfew_shot_clustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster_acc\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Studies\\M2 MLSD\\ppd\\few-shot-clustering\\few_shot_clustering\\wrappers.py:127\u001b[0m, in \u001b[0;36mLLMKeyphraseClustering\u001b[1;34m(features, documents, num_clusters, prompt, text_type, encoder_model, prompt_for_encoder, cache_file)\u001b[0m\n\u001b[0;32m    110\u001b[0m     encoder_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/distilbert-base-nli-stsb-mean-tokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m clusterer \u001b[38;5;241m=\u001b[39m GPTExpansionClustering(features,\n\u001b[0;32m    113\u001b[0m                                    documents,\n\u001b[0;32m    114\u001b[0m                                    encoder_model\u001b[38;5;241m=\u001b[39mencoder_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m                                    instruction_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    126\u001b[0m                                    demonstration_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 127\u001b[0m \u001b[43mclusterer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clusterer\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\Documents\\Studies\\M2 MLSD\\ppd\\few-shot-clustering\\few_shot_clustering\\active_semi_supervised_clustering\\active_semi_clustering\\semi_supervised\\pairwise_constraints\\gptclustering.py:141\u001b[0m, in \u001b[0;36mGPTExpansionClustering.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    139\u001b[0m                     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (end \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m                 time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_only:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from few_shot_clustering.wrappers import LLMKeyphraseClustering\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "from few_shot_clustering.dataloaders import load_clinc\n",
    "\n",
    "# You can provide an optional file to cache the extracted features, \n",
    "# since these are a bit expensive to compute. Example:\n",
    "# cache_path = \"/tmp/clinc_feature_cache.pkl\"\n",
    "#\n",
    "# This is not necessary, as shown below.\n",
    "\n",
    "\n",
    "prompt = \"\"\"I am trying to cluster task-oriented dialog system queries based on whether they express the same general user intent. To help me with this, for a given user query, provide a comprehensive set of keyphrases that could describe this query's intent. These keyphrases should be distinct from those that might describe queries with different intents. Generate the set of keyphrases as a JSON-formatted list.\n",
    "\n",
    "Query: \"how would you say fly in italian\"\n",
    "\n",
    "Keyphrases: [\"translation\", \"translate\"]\n",
    "\n",
    "Query: \"what does assiduous mean\"\n",
    "\n",
    "Keyphrases: [\"definition\", \"define\"]\n",
    "\n",
    "Query: \"find my cellphone for me!\"\n",
    "\n",
    "Keyphrases: [\"location\", \"find\", \"locate\", \"tracking\", \"track\"]\"\"\"\n",
    "\n",
    "cache_path = \"./tmp/clinc_feature_cache.pkl\"\n",
    "features, labels, documents = load_clinc(cache_path)\n",
    "\n",
    "prompt_suffix = \"express the same general intent?\"\n",
    "text_type = \"Query\"\n",
    "encoder_model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "\n",
    "features = features[:100]\n",
    "labels = labels[:100]\n",
    "documents = documents[:100]\n",
    "\n",
    "cluster_assignments = LLMKeyphraseClustering(features, documents, 150, prompt, text_type, encoder_model=encoder_model, prompt_for_encoder=\"Represent keyphrases for topic classification:\", cache_file=\"./tmp/clinc_expansion_cache_file.json\")\n",
    "\n",
    "from few_shot_clustering.eval_utils import cluster_acc\n",
    "import numpy as np\n",
    "print(f\"Accuracy: {cluster_acc(np.array(cluster_assignments), np.array(labels))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0094b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Raouf\\\\Documents\\\\Studies\\\\M2 MLSD\\\\ppd\\\\code'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b57f3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brain fluid buildup delay giffords rehab</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trailer talk week movie rite mechanic week opp...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rnc appoints chairman tampa convention effort ...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gbagbo camp futile cut ivory coast economy</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese president lost translation powerful le...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>live moscow airport explosion</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>supreme court refuse reinstate circuit global ...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>yemeni protester urged president ali abdullah ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>indian navy coast guard rescue thai vessel pir...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>christie prof adept social medium</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2472 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  cluster\n",
       "0              brain fluid buildup delay giffords rehab       37\n",
       "1     trailer talk week movie rite mechanic week opp...       14\n",
       "2     rnc appoints chairman tampa convention effort ...      100\n",
       "3            gbagbo camp futile cut ivory coast economy      110\n",
       "4     chinese president lost translation powerful le...       61\n",
       "...                                                 ...      ...\n",
       "2467                      live moscow airport explosion       36\n",
       "2468  supreme court refuse reinstate circuit global ...       89\n",
       "2469  yemeni protester urged president ali abdullah ...       79\n",
       "2470  indian navy coast guard rescue thai vessel pir...      107\n",
       "2471                  christie prof adept social medium       45\n",
       "\n",
       "[2472 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_json(\"../code/datasets/Tweets.txt\", lines=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9bce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Language Translation, Language Learning, Multilingual Communication', 'Financial, Banking, Transaction', 'Time Management,Productivity,Task Management', 'Definition, Define, Meaning', 'Philosophy,Existential,Meaning', 'Insurance, Policy, Update', 'Location Tracking, Lost Items, Assistance', 'Travel Advisory, Safety Information, Security Concerns', 'Vacation Request, Time Off, Assistance', 'Credit Score Improvement, Financial Impact, Creditworthiness', 'Information, Facts, Interesting', 'Language, Communication, Speak', 'Time Management,Payment Schedule, Salary and Paycheck', 'Delivery, Customer Service, Timeline', 'Time, Clock, Time Zone', 'Application Status, Update, Credit Card', 'Flight Status, Flight Update, Airlines', 'Random, Decision Making, Coin Flip', 'Name Change, Contact, Personalization', 'Origin, Location, Source', 'List Management, Shopping, Grocery', 'Assistance,Support,Capabilities', 'Decision Making,Uncertainty,Communications', 'Car Maintenance, Oil Change, Step-by-step Maintenance', 'Reservation,Booking,Dining', 'Financial, Banking, Balance', 'Reservation,Confirmation,Booking', 'Account Security, Freeze, Block', '401k Transfer,Financial Planning,Retirement Accounts', 'Creator, Developer, Manufacturer']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Unpickle the list from the file\n",
    "with open('../cluster-seed-words/clinc_clusters_seed_words.pkl', 'rb') as f:\n",
    "    my_list = pickle.load(f)\n",
    "\n",
    "print(my_list)  # Output: [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985db13e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Chatbots' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15556\\60079367.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Get the vectors for each keyphrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mkeyphrase_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeyphrase\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyphrases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Calculate the mean vector of all keyphrase vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15556\\60079367.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Get the vectors for each keyphrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mkeyphrase_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeyphrase\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyphrases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Calculate the mean vector of all keyphrase vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'Chatbots' not present\""
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_path = './w2v-model/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "# List of keyphrases\n",
    "og_keyphrases = [\"Learning\", \"Deep Learning\", \"Neural Networks\", \"Natural Language Processing\",\n",
    "              \"Computer Vision\", \"Reinforcement Learning\", \"Data Science\", \"Artificial Neural Networks\",\n",
    "              \"Image Recognition\", \"Speech Recognition\", \"Chatbots\", \"Autonomous Vehicles\", \"Robotics\",\n",
    "              \"Predictive Analytics\", \"Expert Systems\"]\n",
    "keyphrases = []\n",
    "#keyphrases = [keyphrase.split(\" \") if \" \" in keyphrase else keyphrase for keyphrase in og_keyphrases]\n",
    "for keyphrase in og_keyphrases:\n",
    "    if \" \" in keyphrase: \n",
    "        for k in keyphrase.split(\" \"):\n",
    "            keyphrases.append(k)\n",
    "    else:\n",
    "        keyphrases.append(keyphrase)\n",
    "\n",
    "# Get the vectors for each keyphrase\n",
    "keyphrase_vectors = [word2vec_model[keyphrase] for keyphrase in keyphrases]\n",
    "\n",
    "# Calculate the mean vector of all keyphrase vectors\n",
    "mean_vector = sum(keyphrase_vectors) / len(keyphrase_vectors)\n",
    "\n",
    "# Find the most similar keyphrases to the mean vector\n",
    "similar_keyphrases = word2vec_model.similar_by_vector(mean_vector, topn=3)\n",
    "\n",
    "# Extract the similar keyphrases\n",
    "seed_words = [keyphrase for keyphrase, _ in similar_keyphrases]\n",
    "\n",
    "print(\"Seed words for the cluster of keyphrases:\")\n",
    "print(seed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f5a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed words for the cluster of keyphrases:\n",
      "['Imaging_Techniques', 'Neural_Circuits', 'Single_Molecule']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_path = './w2v-model/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "# List of keyphrases\n",
    "keyphrases = [\"Climate Change\", \"Global Warming\", \"Greenhouse Gas Emissions\", \"Renewable Energy\", \"Carbon Footprint\", \"Extreme Weather Events\", \"Sea Level Rise\", \"Deforestation\", \"Melting Ice Caps\", \"Sustainable Development\", \"Climate Action\", \"Mitigation Strategies\", \"Biodiversity Loss\", \"Ocean Acidification\", \"Paris Agreement\"]\n",
    "\n",
    "\n",
    "# Initialize a list to store keyphrase vectors\n",
    "keyphrase_vectors = []\n",
    "\n",
    "# Iterate through each keyphrase\n",
    "for keyphrase in keyphrases:\n",
    "    words = keyphrase.split()  # Split keyphrase into individual words\n",
    "    valid_words = [word for word in words if word in word2vec_model]  # Filter out words not in vocabulary\n",
    "    if valid_words:\n",
    "        # Calculate the average vector for valid words\n",
    "        keyphrase_vector = np.mean([word2vec_model[word] for word in valid_words], axis=0)\n",
    "        keyphrase_vectors.append(keyphrase_vector)\n",
    "\n",
    "# Calculate the mean vector of all valid keyphrase vectors\n",
    "if keyphrase_vectors:\n",
    "    mean_vector = sum(keyphrase_vectors) / len(keyphrase_vectors)\n",
    "\n",
    "    # Find the most similar words to the mean vector\n",
    "    similar_words = word2vec_model.similar_by_vector(mean_vector, topn=3)\n",
    "\n",
    "    # Extract the similar words\n",
    "    seed_words = [word for word, _ in similar_words]\n",
    "\n",
    "    print(\"Seed words for the cluster of keyphrases:\")\n",
    "    print(seed_words)\n",
    "else:\n",
    "    print(\"No valid keyphrase vectors found. Unable to calculate seed words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268576aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
